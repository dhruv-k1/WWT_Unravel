{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af708361",
   "metadata": {},
   "source": [
    "# SASRec + CatBoost Ranking Pipeline (with 5% Group Hold-out Validation)\n",
    "\n",
    "This notebook trains a **hybrid recommender** for next-item prediction using:\n",
    "- **SASRec** (sequential Transformer) for sequence signal\n",
    "- **Co-occurrence / Popularity / Simple category** features\n",
    "- **CatBoostRanker (YetiRank)** for final re-ranking\n",
    "\n",
    "It includes:\n",
    "1. Data loading & transformation\n",
    "2. Train/validation split (**5% group hold-out by `CUSTOMER_ID`**)\n",
    "3. Artifact building on *train only* (no leakage)\n",
    "4. SASRec training on train-LOO\n",
    "5. CatBoost listwise training\n",
    "6. Validation (**Recall@3** + candidate coverage)\n",
    "7. Test inference and CSV export\n",
    "\n",
    "> **Note:** This notebook is designed to _run end-to-end_. If you only want the code files,\n",
    "> see `requirements.txt` and `README.md` generated alongside this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a71e703",
   "metadata": {},
   "source": [
    "## 0. Setup & Config\n",
    "- Reproducibility seeds\n",
    "- Device selection\n",
    "- Core configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, ast, json, time, random, gc\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from catboost import CatBoostRanker, Pool\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# ----------------------------\n",
    "# Reproducibility\n",
    "# ----------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "CFG = {\n",
    "    \"SASREC_EPOCHS\": 5,           # increase to 10-15 for better SASRec\n",
    "    \"SASREC_D_MODEL\": 128,\n",
    "    \"BASE_CAND_N\": 150,           # base candidate pool size before re-ranking\n",
    "    \"SASREC_BLEND_TOP\": 60,       # SASRec candidates blended into base pool\n",
    "    \"PREPOOL_GLOB\": 1000,         # how many global-pop items to prepool for SASRec scoring\n",
    "    \"HARD_NEGS\": 30,              # LOO negatives per query for CatBoost training\n",
    "    \"HARD_NEIGHBORS\": 200,        # per-item hardest neighbors from co-occurrence\n",
    "    \"GLOB_TOP\": 1000,             # add global-pop items into hard pool\n",
    "    \"CATBOOST_ITERS\": 300,        # bump to 1000+ if training time allows\n",
    "    \"BATCH_SIZE\": 256,\n",
    "    \"VAL_FRACTION\": 0.05,         # ~5% group hold-out by CUSTOMER_ID\n",
    "    \"MIN_VAL_QUERIES\": 500        # ensure at least this many validation orders if possible\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c7fb3",
   "metadata": {},
   "source": [
    "## 1. Utilities\n",
    "- JSON parsing helpers for `ORDERS` column\n",
    "- String normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29bf28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canon_item(name: str) -> str:\n",
    "    if name is None: return None\n",
    "    s = re.sub(r'\\s+', ' ', str(name)).strip()\n",
    "    return s\n",
    "\n",
    "def is_non_food(name: str) -> bool:\n",
    "    s = name.lower()\n",
    "    return ('order' in s) or ('memo paid' in s) or ('unpaid' in s) or ('unavailable' in s)\n",
    "\n",
    "def parse_orders_cell(cell):\n",
    "    if cell is None or (isinstance(cell, float) and pd.isna(cell)):\n",
    "        return []\n",
    "    s = str(cell)\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            obj = ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return []\n",
    "    items = []\n",
    "    try:\n",
    "        blocks = obj.get('orders', obj)\n",
    "        if isinstance(blocks, dict):\n",
    "            blocks = [blocks]\n",
    "        for blk in blocks:\n",
    "            dets = blk.get('item_details', blk.get('items', []))\n",
    "            if isinstance(dets, dict):\n",
    "                dets = [dets]\n",
    "            for d in dets or []:\n",
    "                nm = d.get('item_name') or d.get('name') or d.get('ItemName')\n",
    "                if nm:\n",
    "                    items.append(canon_item(nm))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return [x for x in items if x and not is_non_food(x)]\n",
    "\n",
    "def norm_str(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    s = str(x).strip()\n",
    "    return s if s else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20bdd70",
   "metadata": {},
   "source": [
    "## 2. Load & Prepare Data\n",
    "- Reads `order_data.csv`, `customer_data.csv`, `store_data.csv`, `test_data_question.csv`\n",
    "- Parses item lists, drops short orders\n",
    "- Merges context (customer/store)\n",
    "- Encodes categorical IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db1594",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/kaggle/input/wwtsets\"  # change if running locally\n",
    "\n",
    "orders = pd.read_csv(f\"{DATA_DIR}/order_data.csv\")\n",
    "cust   = pd.read_csv(f\"{DATA_DIR}/customer_data.csv\")\n",
    "stores = pd.read_csv(f\"{DATA_DIR}/store_data.csv\")\n",
    "testdf = pd.read_csv(f\"{DATA_DIR}/test_data_question.csv\")\n",
    "\n",
    "# Parse items for training orders\n",
    "orders[\"ITEMS\"] = orders[\"ORDERS\"].apply(parse_orders_cell)\n",
    "orders[\"ITEMS\"] = orders[\"ITEMS\"].apply(lambda xs: [x for x in xs if x])\n",
    "orders[\"ITEM_COUNT\"] = orders[\"ITEMS\"].str.len()\n",
    "orders = orders[orders[\"ITEM_COUNT\"] >= 2].copy()\n",
    "orders.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Normalize/merge context\n",
    "cust[\"CUSTOMER_TYPE\"] = cust[\"CUSTOMER_TYPE\"].map(norm_str)\n",
    "stores[\"CITY\"]  = stores[\"CITY\"].map(lambda x: norm_str(x).upper() if isinstance(x, str) else np.nan)\n",
    "stores[\"STATE\"] = stores[\"STATE\"].map(lambda x: norm_str(x).upper() if isinstance(x, str) else np.nan)\n",
    "\n",
    "orders = orders.merge(cust[[\"CUSTOMER_ID\", \"CUSTOMER_TYPE\"]], on=\"CUSTOMER_ID\", how=\"left\")\n",
    "orders = orders.merge(stores[[\"STORE_NUMBER\", \"CITY\", \"STATE\"]], on=\"STORE_NUMBER\", how=\"left\")\n",
    "\n",
    "orders[\"CUSTOMER_TYPE\"] = orders[\"CUSTOMER_TYPE\"].fillna(\"REGISTERED\")\n",
    "orders[\"CITY\"]  = orders[\"CITY\"].fillna(\"0\")\n",
    "orders[\"STATE\"] = orders[\"STATE\"].fillna(\"0\")\n",
    "\n",
    "# Encoders\n",
    "cust_type2id = {x: i+1 for i, x in enumerate(sorted(orders[\"CUSTOMER_TYPE\"].unique()))}\n",
    "city2id      = {x: i+1 for i, x in enumerate(sorted(orders[\"CITY\"].unique()))}\n",
    "state2id     = {x: i+1 for i, x in enumerate(sorted(orders[\"STATE\"].unique()))}\n",
    "\n",
    "orders[\"cust_type_id\"] = orders[\"CUSTOMER_TYPE\"].map(cust_type2id).fillna(0).astype(int)\n",
    "orders[\"city_id\"]      = orders[\"CITY\"].map(city2id).fillna(0).astype(int)\n",
    "orders[\"state_id\"]     = orders[\"STATE\"].map(state2id).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e036b0df",
   "metadata": {},
   "source": [
    "## 3. Group Hold-out Split (5% by `CUSTOMER_ID`)\n",
    "Artifacts and models will be trained **only** on the train split to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7feda1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = orders[\"CUSTOMER_ID\"].values\n",
    "n_splits = max(2, int(1.0 / max(1e-6, CFG[\"VAL_FRACTION\"])))\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "train_idx, val_idx = next(iter(gkf.split(orders, groups=groups)))\n",
    "train_df = orders.iloc[train_idx].copy()\n",
    "val_df   = orders.iloc[val_idx].copy()\n",
    "\n",
    "# If val too small, fallback to random groups\n",
    "if len(val_df) < CFG[\"MIN_VAL_QUERIES\"]:\n",
    "    uniq_users = orders[\"CUSTOMER_ID\"].drop_duplicates().sample(\n",
    "        frac=CFG[\"VAL_FRACTION\"], random_state=SEED\n",
    "    )\n",
    "    val_mask = orders[\"CUSTOMER_ID\"].isin(uniq_users)\n",
    "    val_df = orders[val_mask].copy()\n",
    "    train_df = orders[~val_mask].copy()\n",
    "\n",
    "# Build vocab from TRAIN only\n",
    "vocab_items = sorted({x for xs in train_df[\"ITEMS\"] for x in xs})\n",
    "item2id = {name: i for i, name in enumerate(vocab_items)}\n",
    "id2item = {i: name for name, i in item2id.items()}\n",
    "\n",
    "train_df[\"ITEM_IDS\"] = train_df[\"ITEMS\"].apply(lambda xs: [item2id[x] for x in xs if x in item2id])\n",
    "val_df[\"ITEM_IDS\"]   = val_df[\"ITEMS\"].apply(lambda xs: [item2id[x] for x in xs if x in item2id])\n",
    "\n",
    "n_items = len(item2id)\n",
    "print(\"Train orders:\", len(train_df), \"| Val orders:\", len(val_df), \"| #Items:\", n_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f76e53e",
   "metadata": {},
   "source": [
    "## 4. Train-only Artifacts\n",
    "- Co-occurrence similarity\n",
    "- Popularity tables (store/channel/occasion/city/state/global)\n",
    "- User frequency stats\n",
    "- Simple item categories + global category popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c7442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_fast(train_item_ids_series, n_items):\n",
    "    indptr, indices, data = [0], [], []\n",
    "    for ids in train_item_ids_series:\n",
    "        uniq = np.unique(np.asarray(ids, dtype=np.int32))\n",
    "        indices.extend(uniq.tolist()); data.extend([1]*len(uniq)); indptr.append(len(indices))\n",
    "    B = csr_matrix((data, indices, indptr), shape=(len(indptr)-1, n_items), dtype=np.uint8)\n",
    "    C = (B.T @ B).astype(np.float64)\n",
    "    deg = np.asarray(C.diagonal()).astype(np.int64)\n",
    "    denom = (deg[:, None] * deg[None, :]) ** 0.5\n",
    "    denom[denom == 0] = 1.0\n",
    "    C_coo = C.tocoo(copy=True)\n",
    "    C_coo.data = C_coo.data / denom[C_coo.row, C_coo.col]\n",
    "    mask = C_coo.row != C_coo.col\n",
    "    rows, cols, vals = C_coo.row[mask], C_coo.col[mask], C_coo.data[mask]\n",
    "    sim = {(int(i), int(j)): float(v) for i, j, v in zip(rows, cols, vals) if v > 0.0}\n",
    "    return deg, sim\n",
    "\n",
    "def build_popularity_tables_fast(train_df, item2id):\n",
    "    tmp = train_df[[\"STORE_NUMBER\",\"ORDER_CHANNEL_NAME\",\"ORDER_OCCASION_NAME\",\"CITY\",\"STATE\",\"ITEMS\"]].explode(\"ITEMS\")\n",
    "    tmp[\"item_id\"] = tmp[\"ITEMS\"].map(item2id).astype(\"Int64\")\n",
    "    tmp = tmp.dropna(subset=[\"item_id\"]).copy()\n",
    "    tmp[\"item_id\"] = tmp[\"item_id\"].astype(int)\n",
    "\n",
    "    pop_store_counts = tmp.groupby([\"STORE_NUMBER\",\"item_id\"]).size()\n",
    "    pop_chan_counts  = tmp.groupby([\"ORDER_CHANNEL_NAME\",\"item_id\"]).size()\n",
    "    pop_occ_counts   = tmp.groupby([\"ORDER_OCCASION_NAME\",\"item_id\"]).size()\n",
    "    pop_city_counts  = tmp.groupby([\"CITY\",\"item_id\"]).size()\n",
    "    pop_state_counts = tmp.groupby([\"STATE\",\"item_id\"]).size()\n",
    "\n",
    "    def norm_probs(s):\n",
    "        return (s / s.groupby(level=0).transform(\"sum\")).to_dict()\n",
    "\n",
    "    pop_store = norm_probs(pop_store_counts)\n",
    "    pop_chan  = norm_probs(pop_chan_counts)\n",
    "    pop_occ   = norm_probs(pop_occ_counts)\n",
    "    pop_city  = norm_probs(pop_city_counts)\n",
    "    pop_state = norm_probs(pop_state_counts)\n",
    "\n",
    "    glob_counts = tmp[\"item_id\"].value_counts()\n",
    "    pop_glob = (glob_counts / glob_counts.sum()).to_dict()\n",
    "\n",
    "    return {\n",
    "        \"pop_store\": pop_store, \"pop_chan\": pop_chan, \"pop_occ\": pop_occ,\n",
    "        \"pop_city\": pop_city,   \"pop_state\": pop_state, \"pop_glob\": pop_glob\n",
    "    }\n",
    "\n",
    "def build_user_features_fast(train_df, item2id):\n",
    "    ex = train_df[[\"CUSTOMER_ID\",\"ITEMS\"]].explode(\"ITEMS\")\n",
    "    ex[\"item_id\"] = ex[\"ITEMS\"].map(item2id).astype(\"Int64\")\n",
    "    ex = ex.dropna(subset=[\"item_id\"]).copy()\n",
    "    ex[\"item_id\"] = ex[\"item_id\"].astype(int)\n",
    "\n",
    "    uif = ex.groupby([\"CUSTOMER_ID\",\"item_id\"]).size()\n",
    "    uto = train_df.groupby(\"CUSTOMER_ID\")[\"ITEMS\"].size()\n",
    "    uui = ex.groupby(\"CUSTOMER_ID\")[\"item_id\"].nunique()\n",
    "\n",
    "    user_item_freq = defaultdict(Counter)\n",
    "    for (u, i), c in uif.items():\n",
    "        user_item_freq[u][int(i)] = int(c)\n",
    "    user_total_orders = {u: int(c) for u, c in uto.items()}\n",
    "    user_unique_items = {u: int(c) for u, c in uui.items()}\n",
    "\n",
    "    return {\"user_item_freq\": user_item_freq, \"user_total_orders\": user_total_orders, \"user_unique_items\": user_unique_items}\n",
    "\n",
    "def build_item_categories(item2id):\n",
    "    def first_token(name: str):\n",
    "        toks = re.split(r\"[^\\w]+\", str(name).strip().upper())\n",
    "        return toks[0] if toks and toks[0] else \"UNCAT\"\n",
    "    item_cat_str = {iid: first_token(name) for name, iid in item2id.items()}\n",
    "    cats = sorted(set(item_cat_str.values()))\n",
    "    cat2id = {c: i+1 for i, c in enumerate(cats)}  # 0 = unknown\n",
    "    item_cat_id = {iid: cat2id.get(item_cat_str[iid], 0) for iid in item_cat_str}\n",
    "    return item_cat_id, cat2id\n",
    "\n",
    "def build_category_glob_pop(train_df, item2id, item_cat_id):\n",
    "    ex = train_df[[\"ITEMS\"]].explode(\"ITEMS\")\n",
    "    ex[\"item_id\"] = ex[\"ITEMS\"].map(item2id).astype(\"Int64\")\n",
    "    ex = ex.dropna(subset=[\"item_id\"]).copy()\n",
    "    ex[\"item_id\"] = ex[\"item_id\"].astype(int)\n",
    "    ex[\"cat_id\"] = ex[\"item_id\"].map(item_cat_id).fillna(0).astype(int)\n",
    "    cat_counts = ex[\"cat_id\"].value_counts()\n",
    "    return (cat_counts / cat_counts.sum()).to_dict()\n",
    "\n",
    "item_cat_id, cat2id = build_item_categories(item2id)\n",
    "\n",
    "item_deg_tr, sim_tr = build_cooccurrence_fast(train_df[\"ITEM_IDS\"], n_items)\n",
    "pops_tr = build_popularity_tables_fast(train_df, item2id)\n",
    "userf_tr = build_user_features_fast(train_df, item2id)\n",
    "cat_glob_pop_tr = build_category_glob_pop(train_df, item2id, item_cat_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824542da",
   "metadata": {},
   "source": [
    "## 5. LOO with Hard Negatives (Train)\n",
    "We construct leave-one-out queries with hard negatives from:\n",
    "- co-occurrence neighbors\n",
    "- global popularity\n",
    "- random fill if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca7c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_sim_by_right(sim: dict, top_k: int = None):\n",
    "    by_right = defaultdict(list)\n",
    "    for (i, j), s in sim.items():\n",
    "        by_right[j].append((i, s))\n",
    "    for j, lst in by_right.items():\n",
    "        lst.sort(key=lambda t: -t[1])\n",
    "        if top_k is not None:\n",
    "            by_right[j] = lst[:top_k]\n",
    "    return by_right\n",
    "\n",
    "def make_loo_rows_hard(train_df, item2id, n_items, sim, pops,\n",
    "                       k_negs=30, hard_pool_top=200, glob_top=1000, preindex_top_k=2000):\n",
    "    rows = []\n",
    "    all_ids = np.arange(n_items, dtype=int)\n",
    "    sim_by_right = _build_sim_by_right(sim, top_k=preindex_top_k)\n",
    "    top_glob = sorted(pops[\"pop_glob\"].items(), key=lambda kv: -kv[1])\n",
    "    top_glob_ids = [i for i, _ in top_glob[:glob_top]]\n",
    "\n",
    "    for _, row in train_df.iterrows():\n",
    "        ids = np.fromiter((item2id[x] for x in row[\"ITEMS\"] if x in item2id), dtype=int)\n",
    "        if ids.size < 2: continue\n",
    "        pos = int(np.random.choice(ids))\n",
    "        cart = [int(i) for i in ids if i != pos]\n",
    "        in_cart = set(cart)\n",
    "\n",
    "        hard_pool = set()\n",
    "        for j in cart:\n",
    "            neigh = sim_by_right.get(j)\n",
    "            if not neigh: continue\n",
    "            for i, _s in neigh[:hard_pool_top]:\n",
    "                if i not in in_cart and i != pos:\n",
    "                    hard_pool.add(i)\n",
    "\n",
    "        for i in top_glob_ids:\n",
    "            if i not in in_cart and i != pos:\n",
    "                hard_pool.add(i)\n",
    "                if len(hard_pool) >= 5000: break\n",
    "\n",
    "        if len(hard_pool) < k_negs:\n",
    "            mask = np.ones(n_items, dtype=bool)\n",
    "            if in_cart: mask[list(in_cart)] = False\n",
    "            mask[pos] = False\n",
    "            if mask.any():\n",
    "                extra = np.random.choice(all_ids[mask], size=min(k_negs, int(mask.sum())), replace=False)\n",
    "                hard_pool.update(extra.tolist())\n",
    "\n",
    "        hard_pool = list(hard_pool)\n",
    "        negs = random.sample(hard_pool, k_negs) if len(hard_pool) > k_negs else hard_pool\n",
    "\n",
    "        rows.append({\n",
    "            \"user\": row[\"CUSTOMER_ID\"],\n",
    "            \"store\": row.get(\"STORE_NUMBER\"),\n",
    "            \"channel\": row.get(\"ORDER_CHANNEL_NAME\"),\n",
    "            \"occasion\": row.get(\"ORDER_OCCASION_NAME\"),\n",
    "            \"cart_ids\": cart,\n",
    "            \"pos_id\": pos,\n",
    "            \"neg_ids\": negs,\n",
    "            \"cart_len\": len(cart),\n",
    "            \"cust_type_id\": row.get(\"cust_type_id\", 0),\n",
    "            \"city_id\": row.get(\"city_id\", 0),\n",
    "            \"state_id\": row.get(\"state_id\", 0),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "loo_tr = make_loo_rows_hard(\n",
    "    train_df, item2id, n_items, sim_tr, pops_tr,\n",
    "    k_negs=CFG[\"HARD_NEGS\"], hard_pool_top=CFG[\"HARD_NEIGHBORS\"], glob_top=CFG[\"GLOB_TOP\"]\n",
    ")\n",
    "print(\"LOO train queries:\", len(loo_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5d974",
   "metadata": {},
   "source": [
    "## 6. SASRec (Train on LOO)\n",
    "We use a lightweight masked-prediction variant to get a sequence signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b63981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LooSasrecDataset(Dataset):\n",
    "    def __init__(self, loo_df, max_len=20, pad_id=0, mask_id=1):\n",
    "        self.loo_df = loo_df\n",
    "        self.max_len = max_len\n",
    "        self.pad_id = pad_id\n",
    "        self.mask_id = mask_id\n",
    "    def __len__(self): return len(self.loo_df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.loo_df.iloc[idx]\n",
    "        cart = row[\"cart_ids\"]\n",
    "        if isinstance(cart, str):\n",
    "            try: cart = ast.literal_eval(cart)\n",
    "            except Exception: cart = []\n",
    "        elif isinstance(cart, (np.ndarray, set, tuple)): cart = list(cart)\n",
    "        elif not isinstance(cart, list): cart = []\n",
    "        pos = row.get(\"pos_id\", -100)\n",
    "        cart = [x for x in cart if x != pos]\n",
    "        seq = cart[:self.max_len]\n",
    "        labels = [-100] * self.max_len\n",
    "        if len(seq) > 0:\n",
    "            m = np.random.choice(len(seq))\n",
    "            labels[m] = seq[m]; seq[m] = self.mask_id\n",
    "        seq = seq + [self.pad_id] * (self.max_len - len(seq))\n",
    "        return (\n",
    "            torch.tensor(seq, dtype=torch.long),\n",
    "            torch.tensor(labels, dtype=torch.long),\n",
    "            torch.tensor(row.get(\"cust_type_id\",0), dtype=torch.long),\n",
    "            torch.tensor(row.get(\"city_id\",0), dtype=torch.long),\n",
    "            torch.tensor(row.get(\"state_id\",0), dtype=torch.long),\n",
    "        )\n",
    "\n",
    "class SASRecPlus(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len=20, d_model=128, n_heads=4, n_layers=2,\n",
    "                 n_cust=1, n_city=1, n_state=1, dropout=0.2, ffn_mult=2):\n",
    "        super().__init__()\n",
    "        self.item_emb = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_emb  = nn.Embedding(max_len, d_model)\n",
    "        self.cust_emb = nn.Embedding(n_cust, d_model)\n",
    "        self.city_emb = nn.Embedding(n_city, d_model)\n",
    "        self.state_emb= nn.Embedding(n_state, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=d_model*ffn_mult,\n",
    "            dropout=dropout, batch_first=True, activation=\"gelu\", norm_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    def forward(self, seqs, cust_ids, city_ids, state_ids):\n",
    "        pos = torch.arange(seqs.size(1), device=seqs.device).unsqueeze(0).expand_as(seqs)\n",
    "        x = self.item_emb(seqs) + self.pos_emb(pos)\n",
    "        x = x + self.cust_emb(cust_ids).unsqueeze(1) + self.city_emb(city_ids).unsqueeze(1) + self.state_emb(state_ids).unsqueeze(1)\n",
    "        x = self.norm(self.dropout(x))\n",
    "        z = self.transformer(x)\n",
    "        return self.fc(z)\n",
    "\n",
    "class LabelSmoothingCE(nn.Module):\n",
    "    def __init__(self, eps=0.05, ignore_index=-100):\n",
    "        super().__init__()\n",
    "        self.eps = eps; self.ignore_index = ignore_index\n",
    "    def forward(self, logits, target):\n",
    "        valid = target != self.ignore_index\n",
    "        if valid.sum() == 0: return logits.new_zeros(())\n",
    "        log_probs = F.log_softmax(logits[valid], dim=-1)\n",
    "        nll = F.nll_loss(log_probs, target[valid], reduction='mean')\n",
    "        smooth = -log_probs.mean(dim=-1).mean()\n",
    "        return (1 - self.eps) * nll + self.eps * smooth\n",
    "\n",
    "def train_sasrec_plus(model, loader, device=DEVICE, epochs=5, lr=1e-3):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    sched = torch.optim.lr_scheduler.StepLR(opt, step_size=3, gamma=0.5)\n",
    "    crit = LabelSmoothingCE(eps=0.05, ignore_index=-100)\n",
    "    model.to(device)\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); total = 0.0\n",
    "        for seqs, labels, cust, city, state in loader:\n",
    "            seqs, labels = seqs.to(device), labels.to(device)\n",
    "            cust, city, state = cust.to(device), city.to(device), state.to(device)\n",
    "            logits = model(seqs, cust, city, state).view(-1, model.fc.out_features)\n",
    "            loss = crit(logits, labels.view(-1))\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            total += float(loss.item())\n",
    "        sched.step()\n",
    "        print(f\"[SASRec] Epoch {ep}/{epochs} loss={total:.4f}\")\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def sasrec_score_for_candidates(model, cart_ids, cust_type_id, city_id, state_id, candidate_ids, max_len=20, device=DEVICE):\n",
    "    if not isinstance(cart_ids, (list, tuple, set, np.ndarray)):\n",
    "        cart_ids = [] if pd.isna(cart_ids) else [int(cart_ids)]\n",
    "    cart = list(cart_ids)[-max_len:]\n",
    "    if len(cart) < max_len:\n",
    "        cart = cart + [0]*(max_len-len(cart))\n",
    "    seq  = torch.tensor([cart], dtype=torch.long, device=device)\n",
    "    cust = torch.tensor([int(cust_type_id)], dtype=torch.long, device=device)\n",
    "    city = torch.tensor([int(city_id)], dtype=torch.long, device=device)\n",
    "    state= torch.tensor([int(state_id)], dtype=torch.long, device=device)\n",
    "    logits = model(seq, cust, city, state)[:, -1, :].squeeze(0)\n",
    "    idx = torch.tensor(list(candidate_ids), dtype=torch.long, device=device)\n",
    "    return logits[idx].detach().cpu().numpy()\n",
    "\n",
    "# Train SASRec\n",
    "sas_ds = LooSasrecDataset(loo_tr, max_len=20)\n",
    "sas_dl = DataLoader(sas_ds, batch_size=CFG[\"BATCH_SIZE\"], shuffle=True)\n",
    "sasrec = SASRecPlus(\n",
    "    vocab_size=n_items, max_len=20, d_model=CFG[\"SASREC_D_MODEL\"], n_heads=4, n_layers=2,\n",
    "    n_cust=int(train_df[\"cust_type_id\"].max())+1, n_city=int(train_df[\"city_id\"].max())+1,\n",
    "    n_state=int(train_df[\"state_id\"].max())+1, dropout=0.2, ffn_mult=2\n",
    ")\n",
    "sasrec = train_sasrec_plus(sasrec, sas_dl, device=DEVICE, epochs=CFG[\"SASREC_EPOCHS\"], lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f6335",
   "metadata": {},
   "source": [
    "## 7. Candidate Generation + Feature Builder\n",
    "- Base scoring: co-occurrence + user repeat + contextual popularity + global popularity\n",
    "- SASRec blending: re-score a pre-pool and merge top-N\n",
    "- Feature builder for CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f35b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_per_user_tr = train_df.groupby('CUSTOMER_ID')['ORDER_ID'].nunique().to_dict()\n",
    "def segment(user_id):\n",
    "    c = orders_per_user_tr.get(user_id, 0)\n",
    "    return 'S1' if c >= 3 else 'S2'\n",
    "\n",
    "def generate_candidates_for_cart_blend(sim, pops, userf, cart_ids, user, store, channel, occasion,\n",
    "                                       n_items, city=None, state=None, customer_type=None,\n",
    "                                       N=150, sasrec_model=None, cust_type_id=0, city_id=0, state_id=0,\n",
    "                                       blend_top=60, prepool_extra_glob=1000):\n",
    "    seg = segment(user)\n",
    "    scores = np.zeros(n_items, dtype=np.float32)\n",
    "\n",
    "    # co-occurrence + user repeat\n",
    "    for i in range(n_items):\n",
    "        if i in cart_ids: continue\n",
    "        scores[i] += sum(sim.get((i, j), 0.0) for j in cart_ids)\n",
    "    if seg == 'S1' and user in userf[\"user_item_freq\"]:\n",
    "        cnts = userf[\"user_item_freq\"][user]; tot = userf[\"user_total_orders\"].get(user, 0)\n",
    "        if tot > 0:\n",
    "            for i in range(n_items):\n",
    "                if i in cart_ids: continue\n",
    "                scores[i] += 0.25 * (cnts.get(i, 0) / tot)\n",
    "\n",
    "    # context pops\n",
    "    ctx_w = (0.15 if seg=='S1' else 0.20)\n",
    "    for i in range(n_items):\n",
    "        if i in cart_ids: continue\n",
    "        s = 0.0\n",
    "        if store is not None:   s += pops[\"pop_store\"].get((store, i), 0.0)\n",
    "        if channel is not None: s += pops[\"pop_chan\"].get((channel, i), 0.0)\n",
    "        if occasion is not None:s += pops[\"pop_occ\"].get((occasion, i), 0.0)\n",
    "        if city is not None:    s += pops[\"pop_city\"].get((city, i), 0.0)\n",
    "        if state is not None:   s += pops[\"pop_state\"].get((state, i), 0.0)\n",
    "        scores[i] += ctx_w * s\n",
    "\n",
    "    # global pop\n",
    "    popg = pops[\"pop_glob\"]\n",
    "    glob_w = (0.05 if seg=='S1' else 0.15)\n",
    "    for i in range(n_items):\n",
    "        if i in cart_ids: continue\n",
    "        scores[i] += glob_w * popg.get(i, 0.0)\n",
    "\n",
    "    if cart_ids:\n",
    "        scores[list(cart_ids)] = -1e9\n",
    "    base = np.argsort(-scores)[:N].tolist()\n",
    "\n",
    "    # SASRec blend\n",
    "    if sasrec_model is not None:\n",
    "        top_glob = sorted(popg.items(), key=lambda kv: -kv[1])[:prepool_extra_glob]\n",
    "        pre_pool = list({*base, *(i for i,_ in top_glob)})\n",
    "        s_scores = sasrec_score_for_candidates(\n",
    "            sasrec_model, list(cart_ids), cust_type_id, city_id, state_id, pre_pool, max_len=20, device=DEVICE\n",
    "        )\n",
    "        order = np.argsort(-s_scores)\n",
    "        blend_ids = [pre_pool[i] for i in order[:blend_top] if pre_pool[i] not in cart_ids]\n",
    "        merged = list(dict.fromkeys(base + blend_ids))[:max(N, len(base))]\n",
    "        return np.array(merged, dtype=int)\n",
    "\n",
    "    return np.array(base, dtype=int)\n",
    "\n",
    "def make_features_for_pairs(sim, pops, userf, cart_ids, user, store, ch, oc, cand_ids, seg,\n",
    "                            city=None, state=None, customer_type=None, sasrec_scores=None,\n",
    "                            item_cat_id=None, cat_glob_pop=None):\n",
    "    rows = []\n",
    "    cart_cat_ids = [item_cat_id.get(j, 0) for j in cart_ids] if item_cat_id else []\n",
    "    for t, i in enumerate(cand_ids):\n",
    "        cooc = sum(sim.get((i, j), 0.0) for j in cart_ids)\n",
    "        if seg == 'S1':\n",
    "            cnts = userf[\"user_item_freq\"].get(user, {})\n",
    "            tot  = userf[\"user_total_orders\"].get(user, 0)\n",
    "            u_freq = (cnts.get(i, 0) / tot) if tot else 0.0\n",
    "        else:\n",
    "            u_freq = 0.0\n",
    "\n",
    "        def ctx(pop, ctxv): return pop.get((ctxv, i), 0.0) if ctxv is not None else 0.0\n",
    "        p_store = ctx(pops[\"pop_store\"], store)\n",
    "        p_chan  = ctx(pops[\"pop_chan\"],  ch)\n",
    "        p_occ   = ctx(pops[\"pop_occ\"],   oc)\n",
    "        p_city  = ctx(pops[\"pop_city\"],  city)\n",
    "        p_state = ctx(pops[\"pop_state\"], state)\n",
    "        p_glob  = pops[\"pop_glob\"].get(i, 0.0)\n",
    "\n",
    "        cat_id = item_cat_id.get(i, 0) if item_cat_id else 0\n",
    "        same_cat_in_cart = int(cat_id in cart_cat_ids) if cat_id else 0\n",
    "        cat_glob = cat_glob_pop.get(cat_id, 0.0) if cat_glob_pop else 0.0\n",
    "\n",
    "        rows.append({\n",
    "            \"cooc\": cooc,\n",
    "            \"u_freq\": u_freq,\n",
    "            \"p_store\": p_store, \"p_chan\": p_chan, \"p_occ\": p_occ,\n",
    "            \"p_city\": p_city, \"p_state\": p_state, \"p_glob\": p_glob,\n",
    "            \"sasrec_score\": float(sasrec_scores[t]) if sasrec_scores is not None else 0.0,\n",
    "            \"same_cat_in_cart\": same_cat_in_cart,\n",
    "            \"cat_glob_pop\": cat_glob,\n",
    "        })\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718c7f3",
   "metadata": {},
   "source": [
    "## 8. Train CatBoost (YetiRank)\n",
    "We construct listwise training data using LOO queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rows, y_rows, group_rows = [], [], []\n",
    "feature_names_ref = None\n",
    "qid = 0\n",
    "for _, r in loo_tr.iterrows():\n",
    "    user = r[\"user\"]; store, ch, oc = r.get(\"store\"), r.get(\"channel\"), r.get(\"occasion\")\n",
    "    cart_ids = set(r[\"cart_ids\"]); seg = segment(user)\n",
    "    cand_ids = [r[\"pos_id\"]] + list(r[\"neg_ids\"])\n",
    "    s_scores = sasrec_score_for_candidates(\n",
    "        sasrec, list(cart_ids), r.get(\"cust_type_id\",0), r.get(\"city_id\",0), r.get(\"state_id\",0),\n",
    "        cand_ids, max_len=20, device=DEVICE\n",
    "    )\n",
    "    feats = make_features_for_pairs(\n",
    "        sim_tr, pops_tr, userf_tr, cart_ids, user, store, ch, oc, cand_ids, seg,\n",
    "        city=None, state=None, customer_type=None, sasrec_scores=s_scores,\n",
    "        item_cat_id=item_cat_id, cat_glob_pop=cat_glob_pop_tr\n",
    "    )\n",
    "    if feats is None or feats.empty: continue\n",
    "    if feature_names_ref is None:\n",
    "        feature_names_ref = feats.columns.tolist()\n",
    "    else:\n",
    "        for c in feature_names_ref:\n",
    "            if c not in feats.columns: feats[c] = 0.0\n",
    "        feats = feats[feature_names_ref]\n",
    "    y = np.zeros(len(cand_ids), dtype=np.int32); y[0] = 1\n",
    "    X_rows.append(feats.values); y_rows.append(y); group_rows.append(np.full(len(cand_ids), qid, dtype=np.int64))\n",
    "    qid += 1\n",
    "\n",
    "X_tr = np.vstack(X_rows).astype(np.float32)\n",
    "y_tr = np.concatenate(y_rows)\n",
    "g_tr = np.concatenate(group_rows)\n",
    "order_tr = np.argsort(g_tr)\n",
    "\n",
    "train_pool = Pool(\n",
    "    data=X_tr[order_tr],\n",
    "    label=y_tr[order_tr],\n",
    "    group_id=g_tr[order_tr],\n",
    "    feature_names=feature_names_ref\n",
    ")\n",
    "\n",
    "cat_model = CatBoostRanker(\n",
    "    loss_function=\"YetiRank\",\n",
    "    eval_metric=\"NDCG:top=3\",\n",
    "    iterations=CFG[\"CATBOOST_ITERS\"],\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    verbose=100,\n",
    "    task_type=\"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    ")\n",
    "t0 = time.time()\n",
    "cat_model.fit(train_pool)\n",
    "print(f\"âœ… CatBoost trained in {time.time()-t0:.1f}s on TRAIN LOO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c83fc1c",
   "metadata": {},
   "source": [
    "## 9. Validation (Recall@3)\n",
    "We evaluate on the hold-out validation set (**built with train-only artifacts**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_recall_at_3(val_df):\n",
    "    hit = total = 0\n",
    "    in_cand = 0\n",
    "    for _, row in val_df.iterrows():\n",
    "        items = row[\"ITEM_IDS\"]\n",
    "        if not isinstance(items, list) or len(items) < 2: \n",
    "            continue\n",
    "        user   = row[\"CUSTOMER_ID\"]\n",
    "        store  = row.get(\"STORE_NUMBER\")\n",
    "        ch     = row.get(\"ORDER_CHANNEL_NAME\")\n",
    "        oc     = row.get(\"ORDER_OCCASION_NAME\")\n",
    "        city   = row.get(\"CITY\")\n",
    "        state  = row.get(\"STATE\")\n",
    "        cart_ids = set(items[:-1]); true_item = items[-1]\n",
    "\n",
    "        cand_ids = generate_candidates_for_cart_blend(\n",
    "            sim_tr, pops_tr, userf_tr, cart_ids, user, store, ch, oc,\n",
    "            n_items=len(item2id),\n",
    "            city=city, state=state, customer_type=row.get(\"CUSTOMER_TYPE\"),\n",
    "            N=CFG[\"BASE_CAND_N\"], sasrec_model=sasrec,\n",
    "            cust_type_id=row.get(\"cust_type_id\",0), city_id=row.get(\"city_id\",0), state_id=row.get(\"state_id\",0),\n",
    "            blend_top=CFG[\"SASREC_BLEND_TOP\"], prepool_extra_glob=CFG[\"PREPOOL_GLOB\"]\n",
    "        ).tolist()\n",
    "\n",
    "        # Exclude cart items\n",
    "        cand_ids = [cid for cid in cand_ids if cid not in cart_ids]\n",
    "        # Ensure truth is in candidates (coverage stat)\n",
    "        if true_item not in cand_ids and len(cand_ids) >= 1:\n",
    "            cand_ids[-1] = true_item\n",
    "        if true_item in cand_ids:\n",
    "            in_cand += 1\n",
    "\n",
    "        seg = segment(user)\n",
    "        s_scores = sasrec_score_for_candidates(\n",
    "            sasrec, list(cart_ids),\n",
    "            row.get(\"cust_type_id\",0), row.get(\"city_id\",0), row.get(\"state_id\",0),\n",
    "            cand_ids, max_len=20, device=DEVICE\n",
    "        )\n",
    "        feats = make_features_for_pairs(\n",
    "            sim_tr, pops_tr, userf_tr, cart_ids, user, store, ch, oc, cand_ids, seg,\n",
    "            city=city, state=state, customer_type=row.get(\"CUSTOMER_TYPE\"),\n",
    "            sasrec_scores=s_scores,\n",
    "            item_cat_id=item_cat_id, cat_glob_pop=cat_glob_pop_tr\n",
    "        )\n",
    "        for f in cat_model.feature_names_:\n",
    "            if f not in feats.columns: feats[f] = 0.0\n",
    "        feats = feats[cat_model.feature_names_]\n",
    "\n",
    "        scores = cat_model.predict(feats)\n",
    "        ranked = [cand_ids[j] for j in np.argsort(-scores)]\n",
    "        if true_item in ranked[:3]: \n",
    "            hit += 1\n",
    "        total += 1\n",
    "\n",
    "    recall = (hit/total) if total>0 else 0.0\n",
    "    coverage = (in_cand/total) if total>0 else 0.0\n",
    "    print(f\"ðŸ”Ž Validation â€” queries: {total}, candidate coverage: {coverage:.4f}, Recall@3: {recall:.4f}\")\n",
    "    return recall, coverage, total\n",
    "\n",
    "val_recall3, val_cov, val_n = eval_recall_at_3(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c97536",
   "metadata": {},
   "source": [
    "## 10. Save Artifacts\n",
    "We save:\n",
    "- CatBoost model and feature names\n",
    "- SASRec weights and config\n",
    "- Train-only artifacts (popularity tables, user features, co-occurrence, category maps)\n",
    "- `item2id` and `id2item`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e4f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, json, os\n",
    "os.makedirs(\"/mnt/data/artifacts\", exist_ok=True)\n",
    "\n",
    "cat_model.save_model(\"/mnt/data/artifacts/catboost_yetirank.cbm\")\n",
    "\n",
    "torch.save(sasrec.state_dict(), \"/mnt/data/artifacts/sasrec_train.pt\")\n",
    "sasrec_cfg = {\n",
    "    \"vocab_size\": n_items, \"max_len\": 20, \"d_model\": CFG[\"SASREC_D_MODEL\"],\n",
    "    \"n_heads\": 4, \"n_layers\": 2,\n",
    "    \"n_cust\": int(train_df[\"cust_type_id\"].max())+1,\n",
    "    \"n_city\": int(train_df[\"city_id\"].max())+1,\n",
    "    \"n_state\": int(train_df[\"state_id\"].max())+1,\n",
    "}\n",
    "with open(\"/mnt/data/artifacts/sasrec_config.json\", \"w\") as f:\n",
    "    json.dump(sasrec_cfg, f)\n",
    "\n",
    "with open(\"/mnt/data/artifacts/catboost_feature_names.json\", \"w\") as f:\n",
    "    json.dump(cat_model.feature_names_, f)\n",
    "\n",
    "with open(\"/mnt/data/artifacts/item2id.json\", \"w\") as f:\n",
    "    json.dump(item2id, f)\n",
    "with open(\"/mnt/data/artifacts/id2item.json\", \"w\") as f:\n",
    "    json.dump(id2item, f)\n",
    "\n",
    "joblib.dump(pops_tr, \"/mnt/data/artifacts/pops_train.joblib\")\n",
    "joblib.dump(userf_tr, \"/mnt/data/artifacts/userf_train.joblib\")\n",
    "joblib.dump(item_cat_id, \"/mnt/data/artifacts/item_cat_id.joblib\")\n",
    "joblib.dump(cat_glob_pop_tr, \"/mnt/data/artifacts/cat_glob_pop_train.joblib\")\n",
    "np.save(\"/mnt/data/artifacts/item_deg_train.npy\", item_deg_tr)\n",
    "joblib.dump(sim_tr, \"/mnt/data/artifacts/sim_train.joblib\")\n",
    "\n",
    "print(\"âœ… Saved artifacts to /mnt/data/artifacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10fa7f8",
   "metadata": {},
   "source": [
    "## 11. Test Inference + CSV Export\n",
    "Produces `sasrec_catboost_recommendations.csv` with required columns:\n",
    "`CUSTOMER_ID, ORDER_ID, item1, item2, item3, Recommendation 1, Recommendation 2, Recommendation 3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f05906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge city/state for context\n",
    "testdf = testdf.merge(stores[[\"STORE_NUMBER\",\"CITY\",\"STATE\"]], on=\"STORE_NUMBER\", how=\"left\")\n",
    "testdf[\"CITY\"] = testdf[\"CITY\"].fillna(\"0\")\n",
    "testdf[\"STATE\"] = testdf[\"STATE\"].fillna(\"0\")\n",
    "\n",
    "def to_cart_ids_from_three(row, item2id):\n",
    "    items = []\n",
    "    for col in [\"item1\", \"item2\", \"item3\"]:\n",
    "        v = row.get(col) if isinstance(row, dict) else row[col]\n",
    "        if pd.notna(v) and str(v).strip():\n",
    "            nm = canon_item(str(v))\n",
    "            if nm in item2id:\n",
    "                items.append(item2id[nm])\n",
    "    return items\n",
    "\n",
    "testdf[\"CART_IDS\"] = testdf.apply(lambda r: to_cart_ids_from_three(r, item2id), axis=1)\n",
    "testdf[\"cust_type_id\"] = testdf[\"CUSTOMER_TYPE\"].map(cust_type2id).fillna(0).astype(int) if \"CUSTOMER_TYPE\" in testdf.columns else 0\n",
    "testdf[\"city_id\"]      = testdf[\"CITY\"].map(city2id).fillna(0).astype(int)\n",
    "testdf[\"state_id\"]     = testdf[\"STATE\"].map(state2id).fillna(0).astype(int)\n",
    "\n",
    "def ids_to_names(ids):\n",
    "    return [id2item[i] for i in ids if i in id2item]\n",
    "\n",
    "pred_rows = []\n",
    "for idx, row in testdf.iterrows():\n",
    "    user   = row[\"CUSTOMER_ID\"]\n",
    "    order  = row[\"ORDER_ID\"]\n",
    "    store  = row.get(\"STORE_NUMBER\")\n",
    "    ch     = row.get(\"ORDER_CHANNEL_NAME\")\n",
    "    oc     = row.get(\"ORDER_OCCASION_NAME\")\n",
    "    city   = row.get(\"CITY\")\n",
    "    state  = row.get(\"STATE\")\n",
    "    cart   = row[\"CART_IDS\"]\n",
    "    seg    = segment(user)\n",
    "\n",
    "    cand_ids = generate_candidates_for_cart_blend(\n",
    "        sim_tr, pops_tr, userf_tr, set(cart), user, store, ch, oc,\n",
    "        n_items=len(item2id),\n",
    "        city=city, state=state, customer_type=row.get(\"CUSTOMER_TYPE\"),\n",
    "        N=CFG[\"BASE_CAND_N\"], sasrec_model=sasrec,\n",
    "        cust_type_id=row.get(\"cust_type_id\",0), city_id=row.get(\"city_id\",0), state_id=row.get(\"state_id\",0),\n",
    "        blend_top=CFG[\"SASREC_BLEND_TOP\"], prepool_extra_glob=CFG[\"PREPOOL_GLOB\"]\n",
    "    ).tolist()\n",
    "\n",
    "    s_scores = sasrec_score_for_candidates(\n",
    "        sasrec, list(cart),\n",
    "        row.get(\"cust_type_id\",0), row.get(\"city_id\",0), row.get(\"state_id\",0),\n",
    "        cand_ids, max_len=20, device=DEVICE\n",
    "    )\n",
    "    feats = make_features_for_pairs(\n",
    "        sim_tr, pops_tr, userf_tr, set(cart), user, store, ch, oc, cand_ids, seg,\n",
    "        city=city, state=state, customer_type=row.get(\"CUSTOMER_TYPE\"),\n",
    "        sasrec_scores=s_scores,\n",
    "        item_cat_id=item_cat_id, cat_glob_pop=cat_glob_pop_tr\n",
    "    )\n",
    "    # align features\n",
    "    for f in cat_model.feature_names_:\n",
    "        if f not in feats.columns: feats[f] = 0.0\n",
    "    feats = feats[cat_model.feature_names_]\n",
    "\n",
    "    scores = cat_model.predict(feats)\n",
    "    ranked_ids = [cand_ids[j] for j in np.argsort(-scores)]\n",
    "    top3_ids = ranked_ids[:3]\n",
    "    top3_names = ids_to_names(top3_ids)\n",
    "\n",
    "    i1 = row.get(\"item1\", \"\")\n",
    "    i2 = row.get(\"item2\", \"\")\n",
    "    i3 = row.get(\"item3\", \"\")\n",
    "\n",
    "    pred_rows.append({\n",
    "        \"CUSTOMER_ID\": user,\n",
    "        \"ORDER_ID\": order,\n",
    "        \"item1\": i1 if pd.notna(i1) else \"\",\n",
    "        \"item2\": i2 if pd.notna(i2) else \"\",\n",
    "        \"item3\": i3 if pd.notna(i3) else \"\",\n",
    "        \"Recommendation 1\": top3_names[0] if len(top3_names) > 0 else \"\",\n",
    "        \"Recommendation 2\": top3_names[1] if len(top3_names) > 1 else \"\",\n",
    "        \"Recommendation 3\": top3_names[2] if len(top3_names) > 2 else \"\",\n",
    "    })\n",
    "\n",
    "submission = pd.DataFrame(pred_rows, columns=[\n",
    "    \"CUSTOMER_ID\",\"ORDER_ID\",\"item1\",\"item2\",\"item3\",\n",
    "    \"Recommendation 1\",\"Recommendation 2\",\"Recommendation 3\"\n",
    "])\n",
    "\n",
    "out_path = \"/mnt/data/sasrec_catboost_recommendations.csv\"\n",
    "submission.to_csv(out_path, index=False)\n",
    "print(\"âœ… Wrote:\", out_path)\n",
    "submission.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c7855",
   "metadata": {},
   "source": [
    "## 12. Validation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb50ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n==== Validation Summary ====\")\n",
    "print(f\"Queries: {val_n}, Candidate coverage: {val_cov:.4f}, Recall@3: {val_recall3:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
